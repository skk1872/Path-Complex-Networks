import os
import sys
import subprocess
import copy
import torch
import torch.optim as optim
import datetime
import random

from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

from lib.data.datasets import ComplexDataset, InMemoryComplexDataset

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(ROOT_DIR)

from lib.helpers.data_helpers import get_dataloaders, load_graph_dataset
from lib.helpers.lr_scheduler import get_lr_scheduler
from lib.utils.random_seed import set_random_seed
from lib.helpers.trainer import Trainer
from lib.helpers.eval_helpers import extract_results_molecular_datasets, extract_results_tu_datasets, extract_results_sr_datasets
from tools.parser import get_parser, validate_args
from lib.helpers.model_helpers import get_complex_model, get_graph_model
from lib.utils.sr_utils import sr_families
from models.complex_models.tu_models import SparseCIN
from models.graph_models.gin_models import GIN0


import wandb

def create_dummy_data():

    num_of_graphs = 500
    result = []

    #mode = "sum"
    #mode = "sum_of_abs"
    mode = "random_y"

    for i in range(num_of_graphs):

        #  num of nodes: random between 10 and 20
        num_of_nodes = (int)(random.uniform(10,20))

        # for each node, initialize a feature, a random number between -1 and 1
        node_features = []
        for j in range(num_of_nodes):
            node_features.append([random.uniform(-1,1)])

        x = torch.tensor(node_features, dtype=torch.float)

        edges = [[],[]]
        for j in range(num_of_nodes):
            for k in range(j+1, num_of_nodes):
                if random.random() < 0.5:
                    edges[0].append(j)
                    edges[1].append(k)

                    edges[0].append(k)
                    edges[1].append(j)
        edge_index = torch.tensor(edges, dtype=torch.long)

        if mode == "sum":
            if x.sum() > 0:
                y = 0
            else:
                y = 1
        elif mode == "sum_of_abs":
            if x.abs().sum() > 5:
                y = 0
            else:
                y = 1
        elif mode == "random_y":
            if random.random() < 0.5:
                y = 0
            else:
                y = 1



        data = Data(x=x, edge_index=edge_index, y=y)

        result.append(data)

    return result

def create_dummy_data2(duplicated: bool):
    num_of_graphs = 500
    if duplicated:
        num_of_graphs = (int)(num_of_graphs / 2)
    result = []

    num_of_nodes = 6
    # graph1 = two triangles
    edges1 = [[0,1,0,2,1,2,0,3,3,4,3,5,4,5],
              [1,0,2,0,2,1,3,0,4,3,5,3,5,4]]

    # graph2 = two squares
    edges2 = [[0,1,1,2,2,3,3,4,4,5,5,0,0,3],
              [1,0,2,1,3,2,4,3,5,4,0,5,3,0]]


    for i in range(num_of_graphs):

        # for each node, initialize a feature, a random number between -1 and 1
        node_features = []
        node_random_feature_red = [random.uniform(-1,1)]
        node_random_feature_blue = [random.uniform(-1, 1)]
        node_features = [node_random_feature_red,
                         node_random_feature_blue,
                         node_random_feature_blue,
                         node_random_feature_red,
                         node_random_feature_blue,
                         node_random_feature_blue,
                         ]

        if duplicated:
            edges = copy.deepcopy(edges1)
            y = 0
            data = Data(x=torch.tensor(node_features, dtype=torch.float),
                        edge_index=torch.tensor(edges, dtype=torch.long),
                        y=y)
            result.append(data)

            edges = copy.deepcopy(edges2)
            y = 1
            data = Data(x=torch.tensor(node_features, dtype=torch.float),
                        edge_index=torch.tensor(edges, dtype=torch.long),
                        y=y)
            result.append(data)

        else:

            if random.random() < 0.5:
                edges = copy.deepcopy(edges1)
                y = 0
            else:
                edges = copy.deepcopy(edges2)
                y = 1

            data = Data(x=torch.tensor(node_features, dtype=torch.float),
                        edge_index=torch.tensor(edges, dtype=torch.long),
                        y=y)

            result.append(data)

    return result

def create_dummy_data3():

    num_of_graphs = 500
    result = []

    #mode = "sum"
    #mode = "sum_of_abs"
    mode = "random_y"

    for i in range(num_of_graphs):

        #  num of nodes: random between 10 and 20
        num_of_nodes = (int)(random.uniform(10,20))

        # for each node, initialize a feature, a random number between -1 and 1
        node_features = []
        for j in range(num_of_nodes):
            node_features.append([1 / num_of_nodes])

        x = torch.tensor(node_features, dtype=torch.float)

        edges = [[],[]]

        if random.random() < 0.5:
            y = 0
            p = 0.7
        else:
            y = 1
            p = 0.3

        for j in range(num_of_nodes):
            for k in range(j+1, num_of_nodes):
                if random.random() < p:
                    edges[0].append(j)
                    edges[1].append(k)

                    edges[0].append(k)
                    edges[1].append(j)
        edge_index = torch.tensor(edges, dtype=torch.long)

        data = Data(x=x, edge_index=edge_index, y=y)

        result.append(data)

    return result

def main(args):
    """The common training and evaluation script used by all the experiments."""

    # Set random seed (default 43)
    set_random_seed(args.seed)

    # set device
    device = torch.device("cuda:" + str(args.device)) if (torch.cuda.is_available() and args.device != -1) else torch.device("cpu")

    # get timestamp to name experiment
    # fold = 1
    # graph_list, train_ids, val_ids, test_ids, num_classes, num_features, num_node_labels = load_graph_dataset(args.dataset, seed=args.seed, fold=fold, max_ring_size=args.max_ring_size)
    #train_loader, valid_loader, test_loader, dataset = get_dataloaders(args, fold=1)

    # graph_list_train = create_dummy_data2(False)
    # graph_list_valid = create_dummy_data2(False)
    # graph_list_test = create_dummy_data2(False)

    graph_list_train = create_dummy_data3()
    graph_list_valid = create_dummy_data3()
    graph_list_test = create_dummy_data3()

    print("ABCDE")
    train_loader = DataLoader(graph_list_train, batch_size = 100, shuffle=True)
    valid_loader = DataLoader(graph_list_valid, batch_size=100, shuffle=True)
    test_loader = DataLoader(graph_list_test, batch_size=100, shuffle=True)

    # instantiate optimiser
    num_features = 1
    num_classes = 2
    use_coboundaries = True
    use_boundaries = True
    disable_graph_norm = False
    fold = 1
    model = GIN0(num_features,  # num_input_features
                 args.num_layers,  # num_layers
                 args.emb_dim,  # hidden
                 num_classes,  # num_classes
                 dropout_rate=args.drop_rate,  # dropout rate
                 nonlinearity=args.nonlinearity,  # nonlinearity
                 readout=args.readout,  # readout
                 ).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=args.lr)

    # instantiate learning rate decay
    scheduler = get_lr_scheduler(args, optimizer)
    result_folder = args.result_folder
    trainer = Trainer(model, args, train_loader, valid_loader, test_loader, optimizer, scheduler, result_folder, None,
                      fold, device)

    # (!) start training/evaluation
    curves = trainer.train()


if __name__ == "__main__":
    passed_args = sys.argv[1:]
    parser = get_parser()
    args = parser.parse_args(copy.copy(passed_args))
    assert args.stop_seed >= args.start_seed
    validate_args(args)
    main(args)
